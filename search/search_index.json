{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Introduction to Kubernetes","text":""},{"location":"#kubernetes-overview","title":"Kubernetes Overview","text":"<p>Open-source conainer orchestration tool designed to automate deploying, scaling, and operating containerized applications.</p> <ul> <li>Kubernetes is a distriubted system. Multiple machines (physical or virtual) maybe used to build a cluster. </li> <li>Kubernetes places containers on machine based on schedules and availability.</li> <li>Kubernetes can move containers as machines are added/removed.</li> <li>Can use different container runtimes. It is container runtime agnostic.</li> <li>Modular, extensive design.</li> </ul>"},{"location":"#using-kubernetes","title":"Using Kubernetes","text":"<ul> <li>Declarative Configuration</li> <li>Deploy Containers fast</li> <li>Configure networking</li> <li>Scale and Expose services</li> </ul>"},{"location":"#kubernetes-for-operations","title":"Kubernetes for Operations","text":"<p>Kubernetes offers multiple features for Operations people.</p> <ul> <li>Automatically recover failed machines</li> <li>Built-in Support for Machine Maintenance</li> <li>Join Clusters with Federation</li> </ul>"},{"location":"#kubernetes-feature-highlights","title":"Kubernetes Feature Highlights","text":"<ul> <li>Automated deployment rollout and rollback</li> <li>Seamless horizontal scaling</li> <li>Secrets Management</li> <li>Service discovery and load balancing</li> <li>Simple log collection</li> <li>Stateful application support</li> <li>Persistent volume management</li> </ul> <p>AWS's alternative of Kubernets: Amazon ECS (Elastic Container Service). This can be managed by your EC2 or by AWS Fargate.</p>"},{"location":"#deploying-kubernetes","title":"Deploying Kubernetes","text":""},{"location":"#single-node-kubernetes-clusters","title":"Single-Node Kubernetes Clusters","text":"<p>Some options to deploy Kubernetes is - </p> <ul> <li>Docker (includes a port for running single-node docker clusters)</li> <li>Minikube </li> <li>Kubeadm (only for Linux machines, will start kubernetes on the host machine, instead of a virtual machine like the prior two methods)</li> </ul>"},{"location":"#single-node-kubernets-clusters-for-continuous-integration","title":"Single-Node Kubernets Clusters for Continuous Integration","text":"<ul> <li>Create ephemeral clusters that start quickly and are in a clean state.</li> <li>Kubernetes-in-Docker (KinD) is made for this case.</li> </ul>"},{"location":"#multi-node-kubernetes-clusters","title":"Multi-Node Kubernetes Clusters","text":"<ul> <li>For production workloads</li> <li>Horizontal scaling</li> <li>Tolerate Node Failures</li> </ul>"},{"location":"#kubernetes-architecture","title":"Kubernetes Architecture","text":"<ul> <li>The top of the Kubernetes architecture is a Cluster. Cluster referes to all of the machines collectively and can be thought of as the entire running system.</li> <li>Nodes are the machines in the cluster. Nodes can be categorized as workers or masters.</li> <li>Worker nodes include software to run containers managed by the Kubernetes control plane.</li> <li>Master nodes run the control plane.</li> <li>The control plane is a set of APIs and software that Kubernetes users interact with. The APIs and software are referred to as master components.</li> </ul>"},{"location":"#scheduling","title":"Scheduling","text":"<p>Control plane schedules containers onto nodes. Scheduling decisions consider required CPU and other factors. Scheduling here refers to the decision process of placing containers onto node.</p>"},{"location":"#kubernetes-pods","title":"Kubernetes Pods","text":"<ul> <li>Pods are group of Containers. Pods may include one or more containers. All containers in a pod run on the same node. </li> <li>Pods are the smallest building block of Kubernetes.</li> <li>More complex and useful abstractions built on top of Pods.</li> </ul>"},{"location":"#kubernetes-services","title":"Kubernetes Services","text":"<ul> <li>Services define networking rules for exposing groups of Pods - To other pods, To the cluster and To the internet</li> </ul>"},{"location":"#kubernetes-deployments","title":"Kubernetes Deployments","text":"<ul> <li>Manage deploying configuration changes to running Pods. They control rollout and rollback of Pods.</li> <li>Horizontal scaling.</li> </ul>"},{"location":"#interacting-with-kubernetes","title":"Interacting with Kubernetes","text":"<ul> <li>The Kubernetes API Server: Modify cluster state information by sending requests to the Kubernetes API server. The API server is a master componentt that acts as the frontend of the server. It is possible but not common to work directly with the API server.</li> <li>Client Libraries: Client libraries can handle the tediousness of working with the REST API for you. It handles authenticating and managing individual REST API requests and responses.</li> <li>Kubernetes Command-Line Tool (Kubectl): This is by far the most common tool to interact with Kubernetes. With this, you can issue high-level commands that are translated into REST API calls. You can also acccess clusters that are both local and remote. Kubectl also manages all kinds of resources, and provides debugging and introspection features.</li> </ul>"},{"location":"#example-kubectl-commands","title":"Example Kubectl commands","text":"<ul> <li><code>kubectl create</code> to create resources (Pods, Services etc.). We can either specify what we want to create in the commands or in the files. The files are usually in <code>.yaml</code> format and are called manifests.</li> <li><code>kubectl delete</code> to delete resources</li> <li><code>kubectl get</code> to get a list of resources of a given type. For example - <code>kubectl get pods</code> gets all the pods in the current namespace</li> <li><code>kubectl describe</code> to get print a detailed info about a resource(s). For example - <code>kubectl describe &lt;resource-type&gt; &lt;resource-name&gt;</code> </li> <li><code>kubectl logs</code> to print container logs</li> </ul>"},{"location":"#pods","title":"Pods","text":"<ul> <li>Basic building blocks in Kubernetes</li> <li>Contain one or more containers</li> <li>Pod containers all share a container network</li> <li>One IP address per pod</li> </ul>"},{"location":"#whats-in-a-pod-declaration","title":"What's in a Pod Declaration","text":"<ul> <li>Container Image</li> <li>Container Ports</li> <li>Container Restart Policy</li> <li>Resource Limits</li> </ul>"},{"location":"#manifest-files","title":"Manifest Files","text":"<p>All kinds of Pod configurations are written in a manifest file, typicall written in <code>.yaml</code>. Manifests can describe all kinds of resources. </p> <ul> <li><code>apiVersion</code> tells kubernetes which api version is going to be used </li> <li><code>kind</code> tells what kind of resource is going to be used</li> <li><code>metadata</code> provides relevant information abou the resource. For example - name of the resource</li> <li><code>spec</code> contains resource specific properties</li> </ul>"},{"location":"#manifests-in-action","title":"Manifests in Action","text":"<p>Manifests are send to kubernetes api server to take actions on. <code>kubectl create</code> sends manifest to Kubernetes API Server.</p> <p>API server does the following for Pod manifests -</p> <ul> <li>Select a node with sufficient resources</li> <li>Schedule Pod onto a node</li> <li>Node pulls Pod's container image</li> <li>Start Pod's container</li> </ul>"},{"location":"#services","title":"Services","text":"<ul> <li>A service defines networking rules for accessing Pods in the cluster and from the internet. You can use labels to select a group of pods.</li> <li>Service has a fixed IP address.</li> <li>Distribute requests across Pods in the group.</li> </ul>"},{"location":"#example-manifest","title":"Example Manifest","text":"<pre><code>apiVersion: v1\nkind: Service\nmetadata:\n    labels:\n        app: webserver\n    name: webserver\nspec:\n    ports:\n    - port: 80\n    selector:\n        app: webserver\n    type: NodePort\n</code></pre>"},{"location":"#multi-container-pods","title":"Multi-Container Pods","text":"<p>To demonstrate this we are going to take an example - </p> <ul> <li>A simple application that increments and prints a count. </li> <li>The application consists 4 containers split across 3 tiers. </li> <li>The Application tier is a Node.js server container. </li> <li>There is a Data tier that contains a Redis cache. </li> <li>The Support tier contains the Poller (that polls the server with GET requests) and Counter (that sends POST requests). </li> <li>All containers configured using the environment variables. </li> </ul>"},{"location":"#kubernetes-namespaces","title":"Kubernetes Namespaces","text":"<ul> <li>We can use namespaces to seperate the resources according to users, environments, or applications.</li> <li>Role-based access control (RBAC) to secure access per Namespace.</li> <li>Using namespace is the best practice.</li> </ul> <p>Example Namespace manifest file -</p> <pre><code>apiVersion: v1\nkind: Namespace\nmetadata:\n    name: microservice\n    labels:\n        app: counter\n</code></pre> <p>Now create Namepace with <code>kubectl create</code> command.</p> <p>Multi Container Application Manifest</p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n    name: app\nspec:\n    containers:\n        - name: redis\n          image: redis:latest\n          imagePullPolicy: IfNotPresent\n          ports:\n            - containerPort: 6379\n\n        - name: server\n          image: \n          ports:\n            - containerPort: 8080\n          env:\n            - name: REDIS_URL\n              value: redis://localhost:6379\n\n        - name: counter\n          image:\n          env:\n            - name: API_URL\n              value: http://localhost:8080\n\n        - name: poller\n          image:\n          env:\n            - name: API_URL\n              value: http://localhost:8080\n</code></pre> <p>Now if you want to create this <code>multi-container pod</code> with the <code>microservice</code> namespace, run this command -</p> <pre><code>kubectl create -f &lt;manifest-name&gt; -n &lt;namespace-name&gt;\n</code></pre> <p>If you want to get the pod, type - <code>kubectl get -n &lt;namespace&gt; pod &lt;pod-name&gt;</code></p>"},{"location":"#service-discovery","title":"Service Discovery","text":"<p>We will take the example that we did in the last lesson. But in this case - the tiers are not in the same pod, but the three tiers are split into three different pods. So they need to communicate. This is where Service comes in.</p> <p>Why Services?</p> <ul> <li>Supports multi-Pod design</li> <li>Provides static endpoint for each tier</li> <li>Handles Pod IP changes</li> <li>They also have Load Balancing. They also distribute loads in the group of pods.</li> </ul> <p>To handle this we need to have App tier service in front of the Server tier and Data tier service infront of the Redis tier.</p> <p>With YAML you can delcare multiple resources in a single manifest by seperating the resources by <code>---</code>. Just like -</p> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n    name: data-tier\n    labels:\n        app: microservices\nspec:\n    ports:\n    - port: 6379\n      protocol: TCP\n      name: redis\n    selector:\n      tier: data\n    type:\n---\napiVersion: v1\nkind: Pod\nmetadata:\n    name: data-tier\n    labels:\n        app: microservices\n        tier: data\nspec:\n    containers:\n        -  name: redis\n           ports:\n               - containerPort: 6379\n\n</code></pre> <p>If you want to create a service for a specific tier, you can do - </p> <pre><code>kubectl describe service -n &lt;service&gt; &lt;tier name&gt;\n</code></pre>"},{"location":"#service-discovery-meachnisms","title":"Service Discovery Meachnisms","text":"<ul> <li>Environment Variables<ul> <li>Services address automatically injected in containers</li> <li>Environment variables follow naming conventions based on service name</li> </ul> </li> <li>DNS <ul> <li>DNS records automatically created in cluster's DNS</li> <li>Containers automatically configured to query cluster DNS</li> </ul> </li> </ul> <p>So in conclusion -</p> <ul> <li>Environment variables and DNS allow Pods to discover Services</li> <li>Environment variables for Services available at Pod creation time and in the same Namespace</li> <li>DNS dynamically updated and accross Namespaces.</li> </ul>"},{"location":"#deployments","title":"Deployments","text":"<ul> <li>Represent multiple replicas of a Pod. Pods in deployment are identical and you can create Pods using the manifest. If a replica is deleted, Kubernetes will automatically create a replica for you</li> <li>Describe a desired state the Kubernetes needs to achieve</li> <li>Deployment Controller master component converges actual state to the desired state</li> <li>Services seamlessly support scaling. Scaling is best with stateless Pods because they support horizontal scaling. This is possible becaues the state of the app is stored in the Data tier.</li> </ul> <p>To get deployments- <code>kubectl get -n &lt;namsepace&gt; deployments</code></p> <p>If we want to scale up certain tiers, we can use the <code>scale</code> command. Here is how we can do this -</p> <pre><code>kubectl scale -n &lt;namespace&gt; deployments &lt;tier-name&gt; --replicas=&lt;number-of-replicas&gt;\n</code></pre>"},{"location":"#autoscaling","title":"Autoscaling","text":"<ul> <li>Scale automatically based on CPU utilization (or custom metrics)</li> <li>Set target CPU along with minimum and max replicas</li> <li>Target CPU is expressed as a percentage of the Pod's CPU request.</li> </ul>"},{"location":"#metrics","title":"Metrics","text":"<ul> <li>Autoscaling depends on metrics being collected</li> <li>Metrics Server is one solution for collecting metrics</li> <li>Several manifest files are used to deploy Metrics server</li> </ul> <p>Autoscaling will need the metrics server to run and collect the metrics. And then autoscaler will increaes or decrease replicas based on the metircs and communicate with the Metrics API.</p>"},{"location":"#rolling-updates-and-rollbacks","title":"Rolling Updates and Rollbacks","text":""},{"location":"#rollouts","title":"Rollouts","text":"<ul> <li>Rollouts update Deployments</li> <li>Any changes to the Deployment template will trigger a rollout</li> </ul>"},{"location":"#rolling-updates","title":"Rolling Updates","text":"<ul> <li>Deployments have different rollout strategies but Kubernetes uses Rolling Updates by default.</li> <li>Replicas are updated in groups, instead of all-at-once, until the rollout is complete. This allows service to continue uninterrupted until the updates rollout.</li> <li>But in Rolling Updates both new and old version are running for some time, so it should be handled gracefully.</li> <li>The alternative of this is the - recreate strategy, where all the running pods are shut down and then deployed again. But this will also increase down time.</li> <li><code>kubectl</code> has commands to check, pause, resume and rollback (undo) rollouts.</li> </ul> <pre><code>spec:\n  strategy:\n    rollingUpdate:\n\n      # how many pods over the desired total (no. of replicas) are allowed during a rollout\n      # a higer surge allows new pods to be created than older pods to be deleted\n      maxSurge: 25% \n\n      # max unavailable tells how many old pods can be deleted without new pods being ready\n      maxUnavailable: 25%\n\n      # the default is 25% for both\n\n    type: RollingUpdate\n</code></pre> <p>You can perform rollouts with <code>kubectl rollout</code> command. For example you can use - <code>kubectl rollout -n deployments status deployment app-tier</code>. Here, <code>-n</code> is used to mention the namespace, <code>status</code> is used to show the status of the rollout and <code>app-tier</code> is the tier name. </p> <p>To pause a rollout you can use - <code>kubectl rollout -n deployments pause deployment app-tier</code>. Pausing a rollout won't delete the already created pods but it will stop the creation of any new pods.</p> <p>To resume a rollout you can use - <code>kubectl rollout -n deployments resume deployment app-tier</code></p>"},{"location":"#rollback","title":"Rollback","text":"<p>If you want to rollback to the previous version, you can use - <code>kubectl rollout -n deployments undo deployment app-tier</code>. Here the <code>undo</code> commands is doing the rollback. If you want to rollback to a specific version then you can use the command - <code>kubectl rollout history -n deployments deployment app-tier</code> and then grab the specific version and pass it into that.</p>"},{"location":"#probes","title":"Probes","text":"<p>Kubernets assumes that the Pods are ready as soon as the containers starts, but that is not always the case. For example, if the container needs some time to warm up, kubernetes should wait before sending any traffic to the pod. It is also possible that the Pod was functional but then it became unresponsive for some reason, maybe it went into a deadlock state, in this case Kubernetes should wait before sending any requests to that Pod.</p> <p>Kubernetes has solutions to all these issues using Probes.</p> <ul> <li>Readiness Probes - Also referred to as Health Checks<ul> <li>Used to check when a Pod is ready to serve traffic/handle requests</li> <li>Useful after startup to chek external dependencies</li> <li>Readiness Probes set the Pod's ready condition. Services only send traffic to ready Pods. Probes integrate with Services to make sure this.</li> </ul> </li> <li>Liveness Probes - Used to detect if a pod has entered into a broken state<ul> <li>Kubernetes will restart the Pod for you. This is the Key difference between Readiness and Liveness probes.</li> <li>Declared in the same way as Readiness probes.</li> </ul> </li> </ul>"},{"location":"#declaring-probes","title":"Declaring Probes","text":"<ul> <li>Probes can be declared in a Pod's container</li> <li>All containers probes must pass for the Pod to pass</li> <li>Probe actions can be a command that runs in the container, an HTTP GET request, or opeaning a TCP socket</li> <li>By default, a probe checks the Pod every 10 seconds</li> </ul>"},{"location":"#init-containers","title":"Init Containers","text":"<p>Sometimes you need to wait for a service, downloads, dynamic or decisions before starting a Pod's containers. The code that checks this could be grounded in the main application. But it is best practice to seperate initialization wait logic from the container Image. However, Initialization is tightly coupled to the main application (belongs to the Pod). </p> <p>So, Kubernetes provides us a way in the form of Init Containers. Init Containers allow you to run initialization tasks before starting the main container(s).</p> <ul> <li>Pods can declare any number of init containers</li> <li>Init containers run in order and run to completion</li> <li>They can use their own images and this can provide some benefits</li> <li>Easy way to block or delay starting an application until some pre-defined conditions are met</li> <li>Run every time a Pod is created</li> </ul>"},{"location":"#volumes","title":"Volumes","text":"<p>Containers in a Pod share their own network stack that each has their own file system. It can be sometimes useful to share data between containers in a  Pod. Lifetime of container file system is limited to the container's lifetime. So this can lead to unexpected consequences if a container restarts.</p>"},{"location":"#pod-storage-in-kubernetes","title":"Pod Storage in Kubernetes","text":"<ul> <li>Two high level storage options: Volumes and Persistent Volumes</li> <li>Used by mounting directory in one or more containers in a Pod</li> <li>Pods can use multiple Volumes and Persistent Volumes</li> <li>Difference between Volumes and Persistent Volumes is how their lifetime is managed. One has a lifetime same to the Pod's lifetime the other one is independent to the Pod's lifetime.</li> </ul>"},{"location":"#volumes_1","title":"Volumes","text":"<ul> <li>Volumes are tied to a pod and their lifecycle.</li> <li>Share data between containers and tolerate container restarts</li> <li>Use for non-durable storage that is deleted with the Pod</li> <li>Default Volume type is <code>emptyDir</code>. Any data in the directory remains when the Pod is restarted but the data is deleted if the Pod is deleted</li> <li>If pod and the data is stored on a specific node but then the pod is restarted on a different node, then the data will be lost</li> </ul>"},{"location":"#persistent-volumes","title":"Persistent Volumes","text":"<ul> <li>Independent of Pod's lifetime</li> <li>Pods claim persistent volumes to use throughout their lifetime.</li> <li>Can be mounted by multiple Pods on different Nodes if underlying storage supports it</li> <li>Can be provisioned statically in advance or dynamically on-demand</li> </ul>"},{"location":"#persistent-volume-claims-pvc","title":"Persistent Volume Claims (PVC)","text":"<p>Pods must send request to a persistent volume to claim it.</p> <p>Persistent volume claims - </p> <ul> <li>Describe a Pod's request for Persistent Volume storage</li> <li>Include how much storage, type of storage and access mode. The access mode describes the volume whether it is mounted in read-only, read-write or read-write many</li> <li>Access modes can be read-write once, read-only many or read-write many</li> <li>PVC stays pending if no PV can satisfy it and dyanmic provisioning is not enabled</li> <li>Connects to a Pod through a Volume of type PVC</li> </ul> <p>Supported Durable Storage: Amazon EBS (Elastic Block Service)</p>"},{"location":"#configmaps-and-secrets","title":"ConfigMaps and Secrets","text":"<p>Up until now, the Deployment template <code>spec</code> has contained all of the configurations required by the Pod. This is better than storing the configuration inside the binary or container image. But adding the configuration in the pod spec can make it a lot less portable. And if the configuration includes sensitive information such as API keys and passwords, it might present a security issue.</p> <p>Kubernetes provides us with ConfigMaps and Secrets to store the Configurations and Secrets. This results in more portable manifests. But the cluster admin also need to ensure that all the proper encryption and safeguards are in place to make sure that the secrets are actually safe. Secrets have specialized types for storing credentials and TLS certs</p>"},{"location":"#using-configmaps-and-secrets","title":"Using ConfigMaps and Secrets","text":"<ul> <li>ConfigMaps and Secrets store data in key-value pairs</li> <li>Pods must reference ConfigMaps and Secrets to use their data</li> <li>Pods can use the data by mounting them as files or through a volume or as environment variables</li> </ul>"},{"location":"#kubernetes-ecosystem","title":"Kubernetes Ecosystem","text":""},{"location":"#helm","title":"Helm","text":"<ul> <li>Helm is Kubernetes Package Manager</li> <li>Packages are called charts and are installed on your cluster using the Helm CLI. We use the Helm CLI to install and udpate charts as it releases on the cluster</li> <li>Charts contain all the resources like deployments and services to run the application</li> <li>Helm charts make it easy to share complete application</li> </ul>"},{"location":"#prometheus","title":"Prometheus","text":"<ul> <li>Open-source monitoring and alerting system</li> <li>A server for pulling time-series metric data and storing it</li> <li>Commonly paired with Grafana for visualizations and Dashboards</li> <li>Define alert rules and send notifications</li> <li>Easily installed via Helm charts</li> </ul>"},{"location":"#pods-in-kube-system-namespace","title":"Pods in kube-system namespace","text":"<pre><code>kubectl get pods --namespace=kube-system\n</code></pre> <p>All of the components of the cluster are running in pods in the kube-system namespace:</p> <ul> <li>aws-cloud-controller: Provides interaction between the cluster and AWS service APIs and allows a cluster to leverage AWS resources.</li> <li>calico: The container network used to connect each node to every other node in the cluster. Calico also supports network policy. Calico is one of many possible container networks that can be used by Kubernetes.</li> <li>coredns: Provides DNS services to nodes in the cluster</li> <li>etcd: The primary data store of all cluster state</li> <li>kube-apiserver: The REST API server for managing the Kubernetes cluster</li> <li>kube-controller-manager: Manager of all of the controllers in the cluster that monitor and change the cluster state when necessary</li> <li>kube-proxy: Network proxy that runs on each node</li> <li>kube-scheduler: Control plane process which assigns Pods to Nodes</li> <li>metrics-server: Not an essential component of a Kubernetes cluster but it is used in this lab to provide metrics for viewing in the Kubernetes dashboard.</li> <li>ebs-csi: Not an essential component of a Kubernetes cluster but is used to manage the lifecycle of Amazon EBS volumes for persistent volumes.</li> </ul>"},{"location":"helm/","title":"Helm","text":""},{"location":"helm/#introduction","title":"Introduction","text":"<p>Helm is the package manager for Kubernetes, used to simplify and enhance the deployment experience for deploying resources into a Kubernetes cluster. </p> <p>But one might be asking why do we need to use <code>Helm</code> if we already have <code>kubectl</code>? The truth is for complex and intricate deployments we need Helm to simplify and enhance the deployment experience.</p> <p>For example, while using <code>kubectl</code> we have to <code>apply</code> the deployment, service, and configMap seperately for each of the tiers. This is not only labor intensive but also prone to error. Depedencies can be forgotten or the installation sequence might be forgotten. Also working with manifests are problematic. There is no parameterization and there is no way to interact with the appilcation lifecycle. All of these creates manifest proliferation. Helm can solve this.</p> <p>Helm has a concept of Charts which is essentially a package containing all the related parts of a specific cluster environment. Helm can be used to deploy applications that consist many resources with a single command. Helm takes care of deploying the chart and all its individual resources in a sequenced order. </p>"},{"location":"helm/#benefits","title":"Benefits","text":"<p>Helm can literally be deployed with a single command. You can also deploy in different environments with different input values that help you customize the overall behaviour of the application. Helm can help you upgrade existing relases, and also keeps track of the release history for you which allows you to rollback to previous release if required. </p> <p>Helm makes it easier to package and distribute charts. After creating the chart you can simply package it up with a chart name. The result is an archive file or chart package which you can host in a chart repository. </p>"},{"location":"helm/#termninology","title":"Termninology","text":"<p>Chart</p> <p>Chart is the package that has a specific file structure, contains many files and templates which when rendered make up the resources that you deploy into your cluster.</p> <p>Repository</p> <p>Repository is an HTTP server that hosts and serves <code>index.yaml</code> file together with one or several packaged charts. </p> <p>Template</p> <p>Templates are used within the chart package that make deployment more general and reusable. By taking a kubernetes manifest and abstracting it into a template. You can also parameterize it, such that while chart installation time, it can take in values that alter the behaviour of the deployed resources. </p> <p>This is useful for deployments that need to take place in multiple environments. Templating your resources helps you reduce and minimize manifest proliferation. </p> <p>Releases</p> <p>When you deploy a chart into a cluster, Helm creates a release for it. A release represent an instance of a chart. The same chart can be used to deploy differently named releases. Releases can be upgraded, rollback and even deleted. </p> <p>A release maintains a history of change where every change being its own revision. </p> <p>Architecture</p> <p>Helm 3 has a client-only architecture. It uses Kubernetes' RBAC - Roll-Based Access Control system to install resources into Pods. Helm directloy communicates with the kubernetes API server and render the templates. Helm 3 also stores release information using Kubernetes Secrets. Whenever a new release happens, a new Kubernetes Secret resource is being created within the same Kubernetes namespace in which the actual deployment took place. </p>"},{"location":"helm/#commands","title":"Commands","text":"<pre><code>helm search hub [KEYWORD] #uses fuzzy based searching for publicly registered charts\nhelm search repo [KEYWORD] #search for local repos, you can add additional repos for search when - help repo add\nhelm pull [CHART]\nhelm install [NAME] [CHART]\nhelm upgrade [RELEASE] [CHART]\nhelm rollback [RELEASE] [REVISION]\nhelm uinstall [RELEASE]\n</code></pre>"},{"location":"helm/#helm-repository-management","title":"Helm Repository Management","text":"<pre><code>helm repo add [NAME] [URL]\nheml repo list\nhelm repo remove [NAME]\nhelm repo update # updates the latest information about charts from the respective chart repositories\nhelm repo index [DIR] # scan the current directory and generate an index file\n</code></pre>"},{"location":"helm/#helm-release-management","title":"Helm Release Management","text":"<pre><code>helm status [RELEASE]\nhelm list\nhelm history [RELEASE]\nhelm get manifest [RELEASE]\n</code></pre>"},{"location":"helm/#helm-chart-management","title":"Helm Chart Management","text":"<pre><code>helm create [NAME]\nhelm template [NAME] [CHART]\nhelm package [CHART]\nhelm lint [CHART]\n</code></pre>"},{"location":"helm/#charts","title":"Charts","text":"<p>By using the command <code>helm create</code> you can get a file structure of the chart. </p> <p>First of all we have the <code>Charts.yaml</code> file where we have top level information of the Chart. </p> <pre><code>apiVersion: \nname: \ndescription:\ntype:\nversion:\nappVersion:\n</code></pre> <p>It must contain a <code>name</code> and a optional <code>description</code> about the chart. The optional <code>type</code> can be either <code>application</code> or <code>library</code>. If the type is <code>application</code> then the chart becomes deployable meaning resources will actually be created in the cluster. A <code>library</code> type means the chart contains reusable functions that can be used in other deployed application charts. The mandatory <code>version</code> field contains the version of the chart itself. The optional <code>appVersion</code> field tracks the version of the application deployed.  </p> <p>The <code>charts</code> folder stores other charts that the current chart is dependent on. During deployed, charts contains in the <code>charts</code> directory are deployed into the cluster. </p> <p>The <code>values.yaml</code> file contains a structured list of default values -</p> <pre><code>replicaCount: 1\n\nimage:\n    repository:\n    pullPolicy:\n\nserviceAccount:\n    create: true\n    annotations: {}\n    name:\n\nservice:\n    type:\n    port:\n</code></pre> <p>These values go to the various templates that reference them like this - <code>{{ .Values.service.type }}</code></p> <p>If you want to override the value stored in the <code>values.yaml</code> file then you can use the <code>install</code> or <code>upgrade</code> command. For example you can use this command - <code>helm upgrade ... \\ --set=services.port=9090</code></p> <p>The <code>templates</code> folder is used to hold all the templates together. Here each template files contains the defenition of a simple cluster resource. A <code>NOTES.txt</code> file can be put into the <code>templates</code> folder that contains end-user instructions on how the deployed application should be accessed once deployed and running within the cluster. </p> <p>The <code>_helpers.tpl</code> contains template partials. Sometimes while creating your own template you can use the same thing multiple times in the same template or differnt template. All of these partials can be stored in a file with an <code>_</code> infront so that it is understood that this is not directly a template. </p> <p>The <code>tests</code> file are written to contain the tests that you wrote to exercies once the templates are generated. Tests are implemented typically using a Kubernetes Job. If the tests exit with a zero exit code then it means that the test is successful and if it exists with a non-zero exit code then the test is not successful. Tests are run by the subocmmand - <code>helm test [RELEASE]</code>. These tests can be used to tests traffic paths, network connections, credentials and/or other specific tasks.</p> <p>After finishing the development of Chart, the next step is to package it using - <code>helm package [CHART DIR]</code>. After packaging you can install the chart using - <code>helm install</code>. If needed you can also do a dry run of a chart installation <code>helm install ... --dry-run</code>.</p> <p>To host a newly created chart into a chart repository, you simply need to create a index chart file using - <code>helm repo index .</code> And then store both the generated <code>index.yaml</code> file together with the chart archive in a directory which is then served up by a standard web server. </p>"},{"location":"helm/#running-a-hosted-chart","title":"Running a Hosted Chart","text":"<pre><code>helm repo add local http://127.0.0.1:8000\nhelm repo update\nhelm search repo cloudcademy\n\nhelm install ca-demo1 local/cloudcademyapp\n</code></pre>"},{"location":"kubernetes-dashboard/","title":"Kubernetes Dashboard Install and Configure","text":"<p>The Kubernetes Dashboard is a web-based Kubernetes user interface. You can use Kubernetes Dashboard to deploy containerized applications to a Kubernetes cluster, troubleshoot your containerized application, and/or manage other cluster resources.</p> <ul> <li>Create a new <code>monitoring</code> namespace within the cluster - </li> </ul> <pre><code>kubectl create ns monitoring\n</code></pre> <ul> <li>Using Helm, install the Kubernetes Dashboard using the publicly available Kubernetes Dashboard Helm Chart. Deploy the dashboard into the monitoring namespace within the lab provided cluster. Use the following command - </li> </ul> <pre><code>{\nhelm repo add k8s-dashboard https://kubernetes.github.io/dashboard\nhelm repo update\nhelm install k8s-dashboard --namespace monitoring k8s-dashboard/kubernetes-dashboard --set=protocolHttp=true --set=serviceAccount.create=true --set=serviceAccount.name=k8sdash-serviceaccount --version 3.0.2\n}\n</code></pre> <ul> <li>Establish permissions within the cluster to allow the Kubernetes Dashboard to read and write all cluster resources -</li> </ul> <pre><code>kubectl create clusterrolebinding kubernetes-dashboard --clusterrole=cluster-admin --serviceaccount=monitoring:k8sdash-serviceaccount\n</code></pre> <ul> <li>The Kubernetes Dashboard web interface now needs to be exposed to the Internet so that you can browse to it. To do so, create a new NodePort based Service, and expose the web admin interface on port 30990. Execute the following command - </li> </ul> <pre><code>{\nkubectl expose deployment k8s-dashboard-kubernetes-dashboard --type=NodePort --name=k8s-dashboard --port=30990 --target-port=9090 -n monitoring\nkubectl patch service k8s-dashboard -n monitoring -p '{\"spec\":{\"ports\":[{\"nodePort\": 30990, \"port\": 30990, \"protocol\": \"TCP\", \"targetPort\": 9090}]}}'\n}\n</code></pre> <ul> <li>Get the public IP address of the Kubernetes cluster that Prometheus has been deployed into (it is in the Flask code). In the terminal execute the following command:</li> </ul> <pre><code>export | grep K8S_CLUSTER_PUBLICIP\n</code></pre>"},{"location":"kustomize/","title":"Kustomize","text":""},{"location":"kustomize/#working-with-kustomize","title":"Working with Kustomize","text":""},{"location":"kustomize/#deploy-basedevelopment","title":"Deploy Base/Development","text":"<p>You can have two directories - base and overlay. The base directory should contain the deployment, service, configMap, ingress and kustomization file for the baseline or development environment. They overlay directory can contain two other directories which are - staging and production.</p> <p>The <code>kustomization.yaml</code> file can look like this - </p> <pre><code>commonLabels:\n app: webapp\n env: base\n version: \"1.02\"\n org: cloudacademy.com\n team: devops.labs\n developer: jeremy.cook\n\nresources:\n- configmap.yaml\n- deployment.yaml\n- service.yaml\n- ingress.yaml \n</code></pre> <p>The <code>commonLabels</code> section defines common metadata labels which get copied into each of the 4 manifest files declared in the <code>resources</code> section when Kustomize is executed.</p> <p>Confirm that you are in the <code>kustomize</code> directory. Use <code>Kustomize</code> to generate and output the set of API resources as declared within the <code>kustomization.yaml</code> file within the <code>base</code> directory. In the terminal execute the following command:</p> <pre><code>kubectl kustomize base\n</code></pre> <p>Now use Kustomize to deploy the same baseline set of resources into the provided Kubernetes cluster. In the terminal execute the following command:</p> <pre><code>kubectl apply -k base\n</code></pre> <p>Confirm that all cluster resources were created successfully. In the terminal execute the following command:</p> <pre><code>kubectl get all\n</code></pre> <p>The sample web application should now be ready to serve Internet based traffic via its assigned FQDN host declared within the base/ingress.yaml file. </p>"},{"location":"kustomize/#deploy-staging","title":"Deploy Staging","text":"<p>The <code>staging</code> directory might contain a <code>kustomization.yaml</code> file like this - </p> <pre><code>namePrefix: stg-\n\ncommonLabels:\n  env: staging\n\ncommonAnnotations:\n  note: staging deployment of cloudacademy lab webapp\n\nbases:\n- ../../base\n\npatchesStrategicMerge:\n- configmap.yaml\n- ingress.yaml\n</code></pre> <ul> <li><code>namePrefix</code> defines a string that is added to the start of all resource names</li> <li><code>commonLabels</code> defines metadata labels that are added to all resources</li> <li><code>commonAnnotations</code> defines metadata annotations that are added to all resources</li> <li><code>bases</code> defines the base directory which is to be merge patched with the resources declared in the <code>patchesStrategicMerge</code> section</li> </ul> <p>Use Kustomize to generate and output the set of API resources as declared within the kustomization.yaml file within the staging (current) directory. In the terminal execute the following command:</p> <pre><code>kubectl kustomize .\n</code></pre> <p>Now use Kustomize to deploy the staging generated set of resources into the Kubernetes cluster. In the terminal execute the following command:</p> <pre><code>kubectl apply -k .\n</code></pre> <p>Confirm that all staging cluster resources were created successfully. In the terminal execute the following command:</p> <pre><code>kubectl get all -l env=staging\n</code></pre>"},{"location":"kustomize/#deploy-production","title":"Deploy Production","text":"<p>You can deploy production the same way you have deployed Staging.</p>"},{"location":"minikube-kubectl-install/","title":"Minikube and Kubectl Install","text":""},{"location":"minikube-kubectl-install/#kubectl","title":"Kubectl","text":"<ul> <li>First check if <code>kubectl</code> is installed - <code>kubectl version --client</code>. If it is not installed, then we have to install <code>kubectl</code>.</li> <li>Download kubectl - <code>curl -LO \"https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl\"</code></li> <li>Validate the binary<ul> <li>Download the kubectl checksum file - <code>curl -LO \"https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl.sha256\"</code></li> <li>Validate the kubectl binary against the checksum file - <code>echo \"$(cat kubectl.sha256)  kubectl\" | sha256sum --check</code></li> <li><code>if [ $? -eq 0 ]; then           echo \"Step 4: Checksum is valid.\"      else           echo \"Step 4: Checksum validation failed. Exiting...\"           exit 1      fi</code></li> </ul> </li> <li>Install kubectl - <code>sudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl</code></li> </ul>"},{"location":"minikube-kubectl-install/#minikube","title":"Minikube","text":"<ul> <li>First check if <code>minikube</code> is installed - <code>minikube version</code></li> <li>Download Minikube - <code>curl -LO https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64</code></li> <li>Install Minikube - <code>sudo install minikube-linux-amd64 /usr/local/bin/minikube &amp;&amp; rm minikube-linux-amd64</code></li> <li>Check if minikube is running - <code>minikube status --format '{{.Host}}' | grep \"Running\"</code></li> </ul>"},{"location":"minikube-kubectl-install/#starting-a-minikube-cluster","title":"Starting a Minikube Cluster","text":"<p>If you want to start a new cluster named <code>test-cluster</code> with 2 nodes (one control-plane and one worker node) -</p> <pre><code>minikube start --nodes 2 -p test-cluster --driver=docker\n</code></pre> <p>To verify if <code>kubectl</code> has <code>kubernetes</code> access - <code>kubectl get nodes</code></p> <p>Get minikube cluster info - <code>kubectl cluster-info</code></p>"}]}